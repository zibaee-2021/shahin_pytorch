{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**MOUNT DRIVE (IF FOR EXAMPLE YOU WANT TO READ/WRITE WEIGHTS FROM MyDrive):**"
   ],
   "metadata": {
    "id": "M0tzs5Wo7Emy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "wNVKH3aT7NaZ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711640775088,
     "user_tz": 0,
     "elapsed": 27132,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "be458525-6b0e-4ca8-a74d-ae8d94034a8c"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Change current working directory from `content` to the directory of the location of this script in `/content/drive/MyDrive/img_cls_to_vit`. This allows the hard-coded relative paths from local machine set up to work here in Colab:**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "evys6N5zIeZE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/img_cls_to_vit'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/img_cls_to_vit')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "id": "9iqAFaoZIeZE",
    "outputId": "b8cf8d60-93a4-4445-c2bd-c9e46c6afcaa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711640775089,
     "user_tz": 0,
     "elapsed": 10,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**INSTALL ALLOWED LIBRARIES:**"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ghc6TQjJIeZF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m64.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m66.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m96.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m4.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m14.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m79.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
      "Pip installed torch, torchvision, pillow and tqdm in 1.27 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pillow\n",
    "!pip install tqdm\n",
    "print(f'Pip installed torch, torchvision, pillow and tqdm in {round((time() - start)/60, 2)} mins')"
   ],
   "metadata": {
    "id": "Io0KXwW-IeZF",
    "outputId": "5d3b93fd-0e53-4cba-bcc2-88ffd24a8734",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711640850770,
     "user_tz": 0,
     "elapsed": 75688,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For TPU, Pip installed torch, torchvision, pillow and tqdm in 1365 secs, i.e. 23 mins. i.e. it's bloody slow!!!\n",
    "\n",
    "\n",
    "For T4 GPU, Pip installed torch, torchvision, pillow and tqdm in 1.27 mins. i.e. it's much faster !!"
   ],
   "metadata": {
    "id": "0RDVWVEGho6-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# FOR COLAB THIS GIVES: Ubuntu 22.04.3 LTS\n",
    "# !cat /etc/*release"
   ],
   "metadata": {
    "id": "31Mv4UI8IeZF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711648425895,
     "user_tz": 0,
     "elapsed": 2,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORT `CIFAR-10` IMAGE TRAINING DATASET,\n",
    "TRANSFORM & LOAD TO DATALOADER & ITERATOR,\n",
    "LOOK AT EXAMPLE OF A BATCH OF IMAGES**<br>\n",
    "(The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It is divided into 50,000 training images and 10,000 test images. The 10 classes are: plane, car, bird, cat, deer, dog, frog, horse, ship & truck.)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ChD6PGrrIeZG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "train_images.png saved.\n",
      "Ground truth labels: frog plane  frog horse   cat   dog  ship  deer  deer   cat   car horse  ship   car   cat horse  bird plane   dog truck\n"
     ]
    }
   ],
   "source": [
    "# CAN SKIP THIS CELL. IT'S JUST TO MAKE A PNG OF 20 EXAMPLE IMAGES\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "batch_size = 20\n",
    "pretrained_transforms = tv_transforms.Compose([\n",
    "    tv_transforms.Resize((224, 224)),\n",
    "    tv_transforms.ToTensor(),\n",
    "    tv_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "trainset = tv_datasets.CIFAR10(root='./data', train=True, download=True, transform=pretrained_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter) # note: for pytorch versions (<1.14) use dataiter.next()\n",
    "\n",
    "# Assuming these are the correct normalization parameters used in your pretrained_transforms\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "# Concatenate batch of images into a single image\n",
    "images_concat = torch.cat(images.split(1, 0), 3).squeeze()\n",
    "# De-normalize\n",
    "for i in range(3):  # Assuming RGB images\n",
    "    images_concat[i] = images_concat[i] * std[i] + mean[i]\n",
    "# Clamp the values to ensure they are between 0 and 1 (this may not be necessary if values are already scaled correctly)\n",
    "images_concat = torch.clamp(images_concat, 0, 1)\n",
    "# Convert to numpy array and then to a PIL Image\n",
    "im = Image.fromarray((images_concat.permute(1, 2, 0).numpy() * 255).astype('uint8'))\n",
    "im.save(\"train_images_corrected.png\")\n",
    "print('train_images.png saved.')\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print('Ground truth labels:' + ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ],
   "metadata": {
    "id": "3PEOB-SyIeZI",
    "outputId": "b65eefe1-d9ce-48ac-b4a0-f2c6d941f17b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711647357179,
     "user_tz": 0,
     "elapsed": 2686,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.datasets as tv_datasets\n",
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps'\n",
    "                      if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')\n",
    "\n",
    "# # may be useful if using CPU\n",
    "# import multiprocessing\n",
    "# cpu_count_mp = multiprocessing.cpu_count()\n",
    "# print(f'cpu count {cpu_count_mp}')\n",
    "# TPU have a CPU count of 40 !"
   ],
   "metadata": {
    "id": "pgU6upJKVUVx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711641455182,
     "user_tz": 0,
     "elapsed": 9,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d12f5641-2b04-4e0c-cbcd-8e7a8f8e571c"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SET FLAG TO TRUE TO DO INFERENCE ONLY"
   ],
   "metadata": {
    "id": "mxQRYOFeN92u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# SET THIS FLAG TO TRUE IF YOU JUST WANT TO DO INFERENCE (AND NOT DO FINE-TUNING OF PRETRAINED VIT MODEL):\n",
    "# load_finetuned_vit_for_inference_only = True\n",
    "load_finetuned_vit_for_inference_only = False\n",
    "\n",
    "if load_finetuned_vit_for_inference_only:\n",
    "    print('You are loading a CIFAR-10-fine-tuned pretrained ViT for inference in testing loop only.')\n",
    "    pretrained_vit = torchvision.models.vit_b_16()\n",
    "    pretrained_vit.heads = nn.Sequential(nn.Linear(in_features=768, out_features=10))\n",
    "    # print('\\nWeights before loading saved model:')\n",
    "    # print(pretrained_vit.heads[0].weight.data)\n",
    "    saved_model_path = 'saved_models/pretrained_finetuned/vit_finetuned.pt'\n",
    "    pretrained_vit.load_state_dict(torch.load(saved_model_path, map_location=torch.device('cuda')))\n",
    "    # print('\\nWeights after loading saved model:')\n",
    "    # print(pretrained_vit.heads[0].weight.data)\n",
    "    pretrained_transforms = tv_transforms.Compose([\n",
    "        tv_transforms.Resize((224, 224)),\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "else:# OTHERWISE TO FINE-TUNE PRETRAINED VIT (ON CIFAR-10): FREEZE WEIGHTS AND THEN ADD TO HEAD:\n",
    "    print('You are loading a pretrained ViT for fine-tuning in training loop (with inference as well in testing loop).')\n",
    "    pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "    pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights)\n",
    "    pretrained_transforms = pretrained_vit_weights.transforms()\n",
    "    # FREEZE MODEL PARAMETERS TO PERFORM TRANSFER LEARNING (I.E FINE-TUNING):\n",
    "    for params in pretrained_vit.parameters():\n",
    "        params.requires_grad=False\n",
    "    pretrained_vit.heads = nn.Sequential(nn.Linear(in_features=768, out_features=10))\n",
    "\n",
    "pretrained_vit.to(device)\n",
    "print('print here to prevent entire architecture print out from previous line')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LX2vf5HJ0e2w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711648783074,
     "user_tz": 0,
     "elapsed": 1567,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "e7dc0411-cc9f-40e0-bf88-595b36f480a6"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You are loading a pretrained ViT for fine-tuning in training loop (with inference as well in testing loop).\n",
      "print here to prevent entire architecture print out from previous line\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TRAIN MODEL FOR 20 EPOCHS:**"
   ],
   "metadata": {
    "id": "tqQ6HETU6NpJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# I PUT test_inference() function and MixUp class inside this cell for debugging purposes, but you can just fold them up\n",
    "def test_inference(pretrained_vit, testloader, criterion):\n",
    "    test_start = time()\n",
    "    pretrained_vit.eval()\n",
    "    test_loss_per_epoch, test_accuracy = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            y_preds = pretrained_vit(inputs)\n",
    "            # y_preds.shape is .  these are   logits.\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "            test_pred_labels = y_preds.argmax(dim=1)\n",
    "            test_accuracy += ((test_pred_labels == labels).sum().item()/len(test_pred_labels))\n",
    "            test_loss_per_epoch += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, test_loss_per_epoch / 2000))\n",
    "                test_loss_per_epoch = 0.0\n",
    "\n",
    "    test_loss_per_epoch = test_loss_per_epoch / len(testloader)\n",
    "    test_accuracy = test_accuracy / len(testloader)\n",
    "    return test_loss_per_epoch, test_accuracy\n",
    "\n",
    "batch_size = 20\n",
    "testset = tv_datasets.CIFAR10(root='./data', train=False, download=True, transform=pretrained_transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(f'load_finetuned_vit_for_inference_only={load_finetuned_vit_for_inference_only}')\n",
    "\n",
    "if load_finetuned_vit_for_inference_only:\n",
    "    # INFERENCE (ON TEST-SET) ONLY:\n",
    "    print(f'One-off inference only')\n",
    "    test_loss, test_acc = test_inference(pretrained_vit, testloader, criterion)\n",
    "\n",
    "else:\n",
    "    # FINE-TUNE AND EVALUATE ON TEST SET FOR 20 EPOCHS:\n",
    "    print('20 epochs of fine-tuning and evalutation on test set per epoch.')\n",
    "    trainset = tv_datasets.CIFAR10(root='./data', train=True, download=True, transform=pretrained_transforms)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    opt = torch.optim.Adam(pretrained_vit.parameters(), lr=0.003)\n",
    "\n",
    "    class MixUp(nn.Module):\n",
    "\n",
    "        def augment(self, device, X, y, batch_size, sampling_method, alpha=0.2):\n",
    "            \"\"\"\n",
    "            If sampling_method is 1: λ is sampled from a beta distribution as described in Zhang et al 2018.\n",
    "            If sampling_method is 2: λ is sampled uniformly from a predefined range.\n",
    "\n",
    "            \"For mixup, we find that αlpha ∈ [0.1, 0.4] leads to improved performance over ERM,\n",
    "            whereas for large αlpha, mixup leads to underfitting.\" Zhang et al.\n",
    "            \"\"\"\n",
    "            np.random.seed(42)\n",
    "\n",
    "            if sampling_method == 2:\n",
    "                lambda_ = np.random.uniform(low=0.0, high=1.0)\n",
    "            else:\n",
    "                lambda_ = np.random.beta(alpha, alpha)\n",
    "\n",
    "            lam = torch.tensor(lambda_, device=device)\n",
    "\n",
    "            random_i = torch.randperm(batch_size).to(device)\n",
    "            X2 = X[random_i, :, :, :]\n",
    "            y2 = y[random_i]\n",
    "\n",
    "            y = F.one_hot(y, num_classes=10) * 1.0\n",
    "            y2 = F.one_hot(y2, num_classes=10) * 1.0\n",
    "            new_X = (lam * X) + ((1. - lam) * X2)\n",
    "            new_y = (lam * y) + ((1. - lam) * y2)\n",
    "\n",
    "            return new_X, new_y\n",
    "\n",
    "    epochs = 20\n",
    "    mix_up = MixUp()\n",
    "\n",
    "    start = time()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'\\nEpoch number {epoch}')\n",
    "        train_loss_per_epoch, test_loss_per_epoch = 0, 0\n",
    "\n",
    "        pretrained_vit.train()\n",
    "\n",
    "        train_losses = torch.zeros(epochs)\n",
    "        test_losses = torch.zeros(epochs)\n",
    "        train_accs = torch.zeros(epochs)\n",
    "        test_accs = torch.zeros(epochs)\n",
    "        train_accuracy, test_accuracy = 0, 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            sampling_method = 1  # 1 FOR UNIFORM\n",
    "            # sampling_method = 2  # 2 for BETA\n",
    "            X, y = mix_up.augment(device=device, X=inputs, y=labels, sampling_method=sampling_method, batch_size=inputs.shape[0])\n",
    "            opt.zero_grad()\n",
    "            y_preds = pretrained_vit(X)\n",
    "            y_preds_class = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n",
    "            y = torch.argmax(y, dim=1) # convert one-hot back to original\n",
    "            train_accuracy += (y_preds_class == y).sum().item()/len(y_preds)\n",
    "            loss = criterion(y_preds, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss_per_epoch += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, train_loss_per_epoch / 2000))\n",
    "\n",
    "        train_loss_per_epoch = train_loss_per_epoch / len(trainloader)\n",
    "        train_accuracy = train_accuracy / len(trainloader)\n",
    "        print(f'train_accuracy {train_accuracy}')\n",
    "\n",
    "        print(f'type(train_loss_per_epoch) {type(train_loss_per_epoch)}')\n",
    "        train_losses[epoch] = train_loss_per_epoch\n",
    "        train_accs[epoch] = train_accuracy\n",
    "        print(f'Epoch {epoch}, training took {round(((time() - start) / 60), 4)} mins')\n",
    "\n",
    "        # 2. EVALUATE ON TEST-SET AFTER EACH EPOCH OF FINE-TUNING:\n",
    "        test_start = time()\n",
    "        test_loss_per_epoch, test_accuracy = test_inference(pretrained_vit, testloader, criterion)\n",
    "\n",
    "        print(f'type(test_loss_per_epoch) {type(test_loss_per_epoch)}')\n",
    "        test_losses[epoch] = test_loss_per_epoch\n",
    "        test_accs[epoch] = test_accuracy\n",
    "        print(f'Epoch {epoch}, test took {round(((time() - test_start) / 60), 4)} mins')\n",
    "\n",
    "    # # SAVE LOSSES & ACCURACIES FOR EACH OF 20 EPOCHS TO CSV FILES:\n",
    "    train_losses_np = train_losses.cpu().numpy()\n",
    "    test_losses_np = test_losses.cpu().numpy()\n",
    "    test_accs_np = test_accs.cpu().numpy()\n",
    "    train_accs_np = train_accs.cpu().numpy()\n",
    "\n",
    "    losses_accs_dirs = f'saved_models/acc_losses/sm_{sampling_method}'\n",
    "    if not os.path.exists(losses_accs_dirs): os.makedirs(losses_accs_dirs)\n",
    "    vit_train_losses_path = os.path.join(losses_accs_dirs, 'train_losses_np.csv')\n",
    "    vit_test_losses_path = os.path.join(losses_accs_dirs, 'test_losses_np.csv')\n",
    "    vit_train_accs_path = os.path.join(losses_accs_dirs, 'train_accs_np.csv')\n",
    "    vit_test_accs_path = os.path.join(losses_accs_dirs, 'test_accs_np.csv')\n",
    "\n",
    "    np.savetxt(vit_test_losses_path, test_losses_np, delimiter=',')\n",
    "    np.savetxt(vit_test_accs_path, test_accs_np, delimiter=',')\n",
    "    np.savetxt(vit_train_losses_path, train_losses_np, delimiter=',')\n",
    "    np.savetxt(vit_train_accs_path, train_accs_np, delimiter=',')\n",
    "\n",
    "    print(f'Classification accuracy per epoch for test set= {test_losses}')\n",
    "    print(f'END - Fine-tuning model for {epochs} epochs took {round(((time() - start) / 60), 4)} mins')\n",
    "\n",
    "# TRUE & TRUE IF YOU'VE JUST FINE-TUNED THE PRETRAINED MODEL ON CIFAR-10 AND WANT TO SAVE THE TRAIN LOSSES & ACCURACIES:\n",
    "\n",
    "# TRUE & TRUE IF YOU'VE JUST FINE-TUNED THE PRETRAINED MODEL ON CIFAR-10\n",
    "# AND YOU WANT TO SAVE THE NEW WEIGHTS (WARNING: OVER-WRITES PATH-FILENAME):\n",
    "save_fine_tuned_model = False\n",
    "if not load_finetuned_vit_for_inference_only:\n",
    "    tuned_model_dirs = f'saved_models/pretrained_finetuned/sm_{sampling_method}'\n",
    "    if not os.path.exists(tuned_model_dirs): os.makedirs(tuned_model_dirs)\n",
    "    fine_tuned_path = os.path.join(tuned_model_dirs, 'vit_finetuned.pt')\n",
    "    torch.save(pretrained_vit.state_dict(), fine_tuned_path)\n",
    "    print('Trained model saved.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-vewmZRo3pd",
    "outputId": "fbcc4ebe-f1b0-4beb-c91a-1ad84525b70c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "load_finetuned_vit_for_inference_only=False\n",
      "20 epochs of fine-tuning and evalutation on test set per epoch.\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Epoch number 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# I PUT test_inference() function and MixUp class inside this cell for debugging purposes, but you can just fold them up\n",
    "def test_inference(pretrained_vit, testloader, criterion):\n",
    "    test_start = time()\n",
    "    pretrained_vit.eval()\n",
    "    test_loss_per_epoch, test_accuracy = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            y_preds = pretrained_vit(inputs)\n",
    "            # y_preds.shape is .  these are   logits.\n",
    "            loss = criterion(y_preds, labels)\n",
    "\n",
    "            test_pred_labels = y_preds.argmax(dim=1)\n",
    "            test_accuracy += ((test_pred_labels == labels).sum().item()/len(test_pred_labels))\n",
    "            test_loss_per_epoch += loss.item()\n",
    "            if i % 2000 == 1999:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, test_loss_per_epoch / 2000))\n",
    "                test_loss_per_epoch = 0.0\n",
    "\n",
    "    test_loss_per_epoch = test_loss_per_epoch / len(testloader)\n",
    "    test_accuracy = test_accuracy / len(testloader)\n",
    "    return test_loss_per_epoch, test_accuracy\n",
    "\n",
    "batch_size = 20\n",
    "testset = tv_datasets.CIFAR10(root='./data', train=False, download=True, transform=pretrained_transforms)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "load_finetuned_vit_for_inference_only = True\n",
    "print(f'load_finetuned_vit_for_inference_only={load_finetuned_vit_for_inference_only}')\n",
    "\n",
    "if load_finetuned_vit_for_inference_only:\n",
    "    # INFERENCE (ON TEST-SET) ONLY:\n",
    "    print(f'One-off inference only')\n",
    "    test_loss, test_acc = test_inference(pretrained_vit, testloader, criterion)\n",
    "\n",
    "else:\n",
    "    # FINE-TUNE AND EVALUATE ON TEST SET FOR 20 EPOCHS:\n",
    "    print('20 epochs of fine-tuning and evalutation on test set per epoch.')\n",
    "    trainset = tv_datasets.CIFAR10(root='./data', train=True, download=True, transform=pretrained_transforms)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    opt = torch.optim.Adam(pretrained_vit.parameters(), lr=0.003)\n",
    "\n",
    "    class MixUp(nn.Module):\n",
    "\n",
    "        def augment(self, device, X, y, batch_size, sampling_method, alpha=0.2):\n",
    "            \"\"\"\n",
    "            If sampling_method is 1: λ is sampled from a beta distribution as described in Zhang et al 2018.\n",
    "            If sampling_method is 2: λ is sampled uniformly from a predefined range.\n",
    "\n",
    "            \"For mixup, we find that αlpha ∈ [0.1, 0.4] leads to improved performance over ERM,\n",
    "            whereas for large αlpha, mixup leads to underfitting.\" Zhang et al.\n",
    "            \"\"\"\n",
    "            np.random.seed(42)\n",
    "\n",
    "            if sampling_method == 2:\n",
    "                lambda_ = np.random.uniform(low=0.0, high=1.0)\n",
    "            else:\n",
    "                lambda_ = np.random.beta(alpha, alpha)\n",
    "\n",
    "            lam = torch.tensor(lambda_, device=device)\n",
    "\n",
    "            random_i = torch.randperm(batch_size).to(device)\n",
    "            X2 = X[random_i, :, :, :]\n",
    "            y2 = y[random_i]\n",
    "\n",
    "            y = F.one_hot(y, num_classes=10) * 1.0\n",
    "            y2 = F.one_hot(y2, num_classes=10) * 1.0\n",
    "            new_X = (lam * X) + ((1. - lam) * X2)\n",
    "            new_y = (lam * y) + ((1. - lam) * y2)\n",
    "\n",
    "            return new_X, new_y\n",
    "\n",
    "    epochs = 20\n",
    "    mix_up = MixUp()\n",
    "\n",
    "    start = time()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f'\\nEpoch number {epoch}')\n",
    "        train_loss_per_epoch, test_loss_per_epoch = 0, 0\n",
    "\n",
    "        pretrained_vit.train()\n",
    "\n",
    "        train_losses = torch.zeros(epochs)\n",
    "        test_losses = torch.zeros(epochs)\n",
    "        train_accs = torch.zeros(epochs)\n",
    "        test_accs = torch.zeros(epochs)\n",
    "        train_accuracy, test_accuracy = 0, 0\n",
    "\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # sampling_method = 1  # 1 FOR UNIFORM\n",
    "            sampling_method = 2  # 2 for BETA\n",
    "            X, y = mix_up.augment(device=device, X=inputs, y=labels, sampling_method=sampling_method, batch_size=inputs.shape[0])\n",
    "            opt.zero_grad()\n",
    "            y_preds = pretrained_vit(X)\n",
    "            y_preds_class = torch.argmax(torch.softmax(y_preds, dim=1), dim=1)\n",
    "            y = torch.argmax(y, dim=1) # convert one-hot back to original\n",
    "            train_accuracy += (y_preds_class == y).sum().item()/len(y_preds)\n",
    "            loss = criterion(y_preds, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss_per_epoch += loss.item()\n",
    "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, train_loss_per_epoch / 2000))\n",
    "\n",
    "        train_loss_per_epoch = train_loss_per_epoch / len(trainloader)\n",
    "        train_accuracy = train_accuracy / len(trainloader)\n",
    "        print(f'train_accuracy {train_accuracy}')\n",
    "\n",
    "        print(f'type(train_loss_per_epoch) {type(train_loss_per_epoch)}')\n",
    "        train_losses[epoch] = train_loss_per_epoch\n",
    "        train_accs[epoch] = train_accuracy\n",
    "        print(f'Epoch {epoch}, training took {round(((time() - start) / 60), 4)} mins')\n",
    "\n",
    "        # 2. EVALUATE ON TEST-SET AFTER EACH EPOCH OF FINE-TUNING:\n",
    "        test_start = time()\n",
    "        test_loss_per_epoch, test_accuracy = test_inference(pretrained_vit, testloader, criterion)\n",
    "\n",
    "        print(f'type(test_loss_per_epoch) {type(test_loss_per_epoch)}')\n",
    "        test_losses[epoch] = test_loss_per_epoch\n",
    "        test_accs[epoch] = test_accuracy\n",
    "        print(f'Epoch {epoch}, test took {round(((time() - test_start) / 60), 4)} mins')\n",
    "\n",
    "    # # SAVE LOSSES & ACCURACIES FOR EACH OF 20 EPOCHS TO CSV FILES:\n",
    "    train_losses_np = train_losses.cpu().numpy()\n",
    "    test_losses_np = test_losses.cpu().numpy()\n",
    "    test_accs_np = test_accs.cpu().numpy()\n",
    "    train_accs_np = train_accs.cpu().numpy()\n",
    "\n",
    "    losses_accs_dirs = f'saved_models/acc_losses/sm_{sampling_method}'\n",
    "    if not os.path.exists(losses_accs_dirs): os.makedirs(losses_accs_dirs)\n",
    "    vit_train_losses_path = os.path.join(losses_accs_dirs, 'train_losses_np.csv')\n",
    "    vit_test_losses_path = os.path.join(losses_accs_dirs, 'test_losses_np.csv')\n",
    "    vit_train_accs_path = os.path.join(losses_accs_dirs, 'train_accs_np.csv')\n",
    "    vit_test_accs_path = os.path.join(losses_accs_dirs, 'test_accs_np.csv')\n",
    "\n",
    "    np.savetxt(vit_test_losses_path, test_losses_np, delimiter=',')\n",
    "    np.savetxt(vit_test_accs_path, test_accs_np, delimiter=',')\n",
    "    np.savetxt(vit_train_losses_path, train_losses_np, delimiter=',')\n",
    "    np.savetxt(vit_train_accs_path, train_accs_np, delimiter=',')\n",
    "\n",
    "    print(f'Classification accuracy per epoch for test set= {test_losses}')\n",
    "    print(f'END - Fine-tuning model for {epochs} epochs took {round(((time() - start) / 60), 4)} mins')\n",
    "\n",
    "# TRUE & TRUE IF YOU'VE JUST FINE-TUNED THE PRETRAINED MODEL ON CIFAR-10 AND WANT TO SAVE THE TRAIN LOSSES & ACCURACIES:\n",
    "\n",
    "# TRUE & TRUE IF YOU'VE JUST FINE-TUNED THE PRETRAINED MODEL ON CIFAR-10\n",
    "# AND YOU WANT TO SAVE THE NEW WEIGHTS (WARNING: OVER-WRITES PATH-FILENAME):\n",
    "save_fine_tuned_model = False\n",
    "if not load_finetuned_vit_for_inference_only:\n",
    "    tuned_model_dirs = f'saved_models/pretrained_finetuned/sm_{sampling_method}'\n",
    "    if not os.path.exists(tuned_model_dirs): os.makedirs(tuned_model_dirs)\n",
    "    fine_tuned_path = os.path.join(tuned_model_dirs, 'vit_finetuned.pt')\n",
    "    torch.save(pretrained_vit.state_dict(), fine_tuned_path)\n",
    "    print('Trained model saved.')"
   ],
   "metadata": {
    "id": "OWlv8ZyyNb-4"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "adl_cw_pt",
   "language": "python",
   "display_name": "comp0197-cw1-pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
