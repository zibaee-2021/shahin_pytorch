{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**MOUNT DRIVE (IF FOR EXAMPLE YOU WANT TO READ/WRITE WEIGHTS FROM MyDrive):**"
   ],
   "metadata": {
    "id": "M0tzs5Wo7Emy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNVKH3aT7NaZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710861382742,
     "user_tz": 0,
     "elapsed": 26473,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "05edef9a-a9b1-4aba-951d-bde96b70734f"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Change current working directory from `content` to the directory of the location of this script in `/content/drive/MyDrive/img_cls_to_vit`. This allows the hard-coded relative paths from local machine set up to work here in Colab:**"
   ],
   "metadata": {
    "id": "8dQsmaJz2HMT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/img_cls_to_vit')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9sqeriFOnaW1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710861382742,
     "user_tz": 0,
     "elapsed": 13,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "4169e8d0-05fc-49c8-a88a-18c0d40fdc6f"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/content/drive/MyDrive/img_cls_to_vit'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**INSTALL ALLOWED LIBRARIES (~80 secs):**"
   ],
   "metadata": {
    "id": "snZGY2jX3Rul"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m58.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m49.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m77.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m1.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m14.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m15.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m82.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchvision) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Pip installed torch, torchvision, pillow and tqdm in 108.9257 secs\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pillow\n",
    "!pip install tqdm\n",
    "!pip install transformers # will need to remove later\n",
    "print(f'Pip installed torch, torchvision, pillow and tqdm in {round(time() - start, 4)} secs')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJTrL2FMgHn0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710861491758,
     "user_tz": 0,
     "elapsed": 109025,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "c4a396b7-158c-4c28-f7e8-e693fd280fab"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !cat /etc/*release"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pjk0uIsV7Mt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710426221369,
     "user_tz": 0,
     "elapsed": 14,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "6e35ea2c-dc09-4994-cda2-13ced9494b32"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORT LIBRARIES AND SET THE PROCESSOR DEVICE:**"
   ],
   "metadata": {
    "id": "_MsHI6pm3xTN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# train script\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tv_transforms\n",
    "import torchvision.datasets as tv_datasets\n",
    "from PIL import Image\n",
    "from mix_up import MixUp\n",
    "# import multiprocessing\n",
    "\n",
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set device: to either Cuda (GPU), MPS (Apple Silicon GPU), or CPU\n",
    "    \"\"\"\n",
    "    device = torch.device(\n",
    "        'cuda'\n",
    "        if torch.cuda.is_available()\n",
    "        else 'mps'\n",
    "        if torch.backends.mps.is_available()\n",
    "        else 'cpu'\n",
    "    )\n",
    "    print(f'Using {device} device')\n",
    "    return device\n",
    "device = set_device()"
   ],
   "metadata": {
    "id": "ypF121EdgHn4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710861499958,
     "user_tz": 0,
     "elapsed": 8205,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "74a96c80-c819-40f4-fa7f-6bcac5bfc423"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMPORT `CIFAR-10` IMAGE TRAINING DATASET, TRANSFORM, LOAD TO DATALOADER & ITERATOR, LOOK AT EXAMPLE (~ 3 sec)**<br>\n",
    "(The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. It is divided into 50,000 training images and 10,000 test images. The 10 classes are: plane, car, bird, cat, deer, dog, frog, horse, ship & truck.)"
   ],
   "metadata": {
    "id": "FvEBKY2k4ELp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights)\n",
    "pretrained_transforms = pretrained_vit_weights.transforms()\n",
    "\n",
    "for params in pretrained_vit.parameters():\n",
    "    params.requires_grad=False\n",
    "\n",
    "pretrained_vit.heads = nn.Sequential(nn.Linear(in_features=768, out_features=10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─Conv2d: 1-1                                 [-1, 768, 14, 14]         (590,592)\n",
      "├─Encoder: 1-2                                [-1, 197, 768]            --\n",
      "|    └─Dropout: 2-1                           [-1, 197, 768]            --\n",
      "|    └─Sequential: 2-2                        [-1, 197, 768]            --\n",
      "|    |    └─EncoderBlock: 3-1                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-2                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-3                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-4                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-5                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-6                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-7                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-8                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-9                 [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-10                [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-11                [-1, 197, 768]            (7,087,872)\n",
      "|    |    └─EncoderBlock: 3-12                [-1, 197, 768]            (7,087,872)\n",
      "|    └─LayerNorm: 2-3                         [-1, 197, 768]            (1,536)\n",
      "├─Sequential: 1-3                             [-1, 10]                  --\n",
      "|    └─Linear: 2-4                            [-1, 10]                  7,690\n",
      "===============================================================================================\n",
      "Total params: 85,654,282\n",
      "Trainable params: 7,690\n",
      "Non-trainable params: 85,646,592\n",
      "Total mult-adds (M): 455.43\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 30.01\n",
      "Params size (MB): 326.75\n",
      "Estimated Total Size (MB): 357.33\n",
      "===============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "===============================================================================================\nLayer (type:depth-idx)                        Output Shape              Param #\n===============================================================================================\n├─Conv2d: 1-1                                 [-1, 768, 14, 14]         (590,592)\n├─Encoder: 1-2                                [-1, 197, 768]            --\n|    └─Dropout: 2-1                           [-1, 197, 768]            --\n|    └─Sequential: 2-2                        [-1, 197, 768]            --\n|    |    └─EncoderBlock: 3-1                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-2                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-3                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-4                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-5                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-6                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-7                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-8                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-9                 [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-10                [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-11                [-1, 197, 768]            (7,087,872)\n|    |    └─EncoderBlock: 3-12                [-1, 197, 768]            (7,087,872)\n|    └─LayerNorm: 2-3                         [-1, 197, 768]            (1,536)\n├─Sequential: 1-3                             [-1, 10]                  --\n|    └─Linear: 2-4                            [-1, 10]                  7,690\n===============================================================================================\nTotal params: 85,654,282\nTrainable params: 7,690\nNon-trainable params: 85,646,592\nTotal mult-adds (M): 455.43\n===============================================================================================\nInput size (MB): 0.57\nForward/backward pass size (MB): 30.01\nParams size (MB): 326.75\nEstimated Total Size (MB): 357.33\n==============================================================================================="
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torch-summary\n",
    "from torchsummary import summary\n",
    "summary(pretrained_vit, (3, 224, 224))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "train_pt_images_vit.jpg saved.\n",
      "Ground truth labels:truck horse  ship plane  bird   car  bird  bird  ship  frog  ship   cat   cat   dog horse  deer horse  frog horse   dog\n"
     ]
    }
   ],
   "source": [
    "# # ViT: transform cifar-10 dataset for 'facebook/deit-...-patch16-224'\n",
    "transform = tv_transforms.Compose([\n",
    "    tv_transforms.Resize((224, 224)),\n",
    "    tv_transforms.ToTensor(),\n",
    "    tv_transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# ViT: Apply transformation to CIFAR10 dataset fr training (50,000 images):\n",
    "# trainset = tv_datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainset = tv_datasets.CIFAR10(root='./data', train=True, download=True, transform=pretrained_transforms)\n",
    "testset = tv_datasets.CIFAR10(root='./data', train=False, download=True, transform=pretrained_transforms)\n",
    "\n",
    "assert len(trainset) == 50000\n",
    "batch_size = 20\n",
    "# cpu_count_mp = multiprocessing.cpu_count()\n",
    "# print(f\"Number of CPU threads according to multiprocessing: {cpu_count_mp}\")\n",
    "# Load in Dataloader:\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=cpu_count_mp)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "# List all 10 classes you can expect to find in CIFAR-10:\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "dataiter = iter(trainloader)  # Create iterator of images\n",
    "images, labels = next(dataiter)  # take one example batch of images out to sanity check.\n",
    "# Save example images to jpg:\n",
    "im = Image.fromarray((torch.cat(images.split(1, 0), 3)\n",
    "                      .squeeze() / 2 * 255 + .5 * 255)\n",
    "                     .permute(1, 2, 0).numpy().astype('uint8'))\n",
    "im.save('train_pt_images_vit.jpg')\n",
    "print('train_pt_images_vit.jpg saved.')\n",
    "print('Ground truth labels:' + ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(ONLY NEED TO DO THIS ONCE)<br>\n",
    "LOAD PRE-TRAINED DeiT TINY ViT MODEL FROM HUGGINGFACE AND SAVED TO LOCAL `.PT` (~1 sec) FILE:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ViT: instantiate pretrained model\n",
    "# !pip install transformers\n",
    "# from transformers import DeiTConfig, DeiTModel\n",
    "# start = time()\n",
    "# from google.colab import userdata\n",
    "# userdata.get('shahin_HF_datasets')\n",
    "\n",
    "# The model is initially installed from HF via `transformers` library,\n",
    "# but subsequently saved to `saved_models/pretrained` dir as `transformers`\n",
    "# is not allowed.\n",
    "\n",
    "# # Init DeiT `deit-tiny-patch16-224` style configuration, (with random weights)\n",
    "# config = DeiTConfig.from_pretrained('facebook/deit-tiny-patch16-224')\n",
    "# config = DeiTConfig.from_pretrained('facebook/deit-small-patch16-224')\n",
    "# net = DeiTModel(config)\n",
    "# # torch.save(net, 'saved_models/pretrained/deit_tiny_vit.pt')\n",
    "# torch.save(net, 'saved_models/pretrained/deit_small_vit.pt')\n",
    "# # Save model architecture to file:\n",
    "# with open('network_pt_deit_small.py', 'w') as f:\n",
    "#     f.write(str(net))\n",
    "# print(f'Pretrained DeiT model uploaded in {round(time() - start, 4)} secs')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/img_cls_to_vit')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LOAD PRE-TRAINED DeiT TINY ViT MODEL FROM LOCAL .PT FILE:**"
   ],
   "metadata": {
    "id": "FmOKgw0f0_Im"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# net_tiny = torch.load(f='saved_models/pretrained/deit_tiny_vit.pt')\n",
    "# net_tiny = net_tiny.to(device)\n",
    "# print(f'Local pretrained DeiT model uploaded in {round(time() - start, 2)} secs')\n",
    "# print(f'DeiT tiny network architecture {net_tiny}')"
   ],
   "metadata": {
    "id": "Rhx1GSfUnFsh",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1710861499960,
     "user_tz": 0,
     "elapsed": 8,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local pretrained DeiT model uploaded in 76.12 secs\n",
      "DeiT tiny network architecture DeiTModel(\n",
      "  (embeddings): DeiTEmbeddings(\n",
      "    (patch_embeddings): DeiTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): DeiTEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x DeiTLayer(\n",
      "        (attention): DeiTAttention(\n",
      "          (attention): DeiTSelfAttention(\n",
      "            (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): DeiTSelfOutput(\n",
      "            (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): DeiTIntermediate(\n",
      "          (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): DeiTOutput(\n",
      "          (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
      "  (pooler): DeiTPooler(\n",
      "    (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local pretrained DeiT model uploaded in 67.3 secs\n",
      "DeiT network architecture DeiTModel(\n",
      "  (embeddings): DeiTEmbeddings(\n",
      "    (patch_embeddings): DeiTPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): DeiTEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x DeiTLayer(\n",
      "        (attention): DeiTAttention(\n",
      "          (attention): DeiTSelfAttention(\n",
      "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (output): DeiTSelfOutput(\n",
      "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): DeiTIntermediate(\n",
      "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): DeiTOutput(\n",
      "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "  (pooler): DeiTPooler(\n",
      "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # net = torch.load(f='saved_models/pretrained/deit_tiny_vit.pt')\n",
    "# net_small = torch.load(f='saved_models/pretrained/deit_small_vit.pt')\n",
    "# net_small = net_small.to(device)\n",
    "# print(f'Local pretrained DeiT model uploaded in {round(time() - start, 2)} secs')\n",
    "# print(f'DeiT network architecture {net_small}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**INITIALISE CROSS ENTROPY LOSS FUNCTION AND SGD OPTIMISER:**"
   ],
   "metadata": {
    "id": "OOdpht3u6aC0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "# optimiser = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimiser = torch.optim.Adam(pretrained_vit.parameters(), lr=0.001)"
   ],
   "metadata": {
    "id": "2_bmQ1fnpbSJ",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1710861499960,
     "user_tz": 0,
     "elapsed": 8,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LOAD PARTIALLY TRAINED MODEL IF PRESENT:**"
   ],
   "metadata": {
    "id": "cPdVnkX46Un-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # Loading model and optimiser state:\n",
    "# checkpoint = torch.load('checkpoint.pth')\n",
    "# net.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']"
   ],
   "metadata": {
    "id": "rJPSC9LNqBhz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _calc_accuracy(predicted_class, ground_truth):\n",
    "    y_pred_class = torch.argmax(torch.softmax(predicted_class, dim=1), dim=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TRAIN MODEL FOR 20 EPOCHS:**"
   ],
   "metadata": {
    "id": "tqQ6HETU6NpJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from transformers import AutoFeatureExtractor\n",
    "# feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/deit-small-patch16-224')\n",
    "start = time()\n",
    "epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    pretrained_vit.train()\n",
    "    loss, cumulative_loss = 0.0, 0.0\n",
    "    # for i, data in enumerate(trainloader, 0):\n",
    "    #         # data is list of [inputs, labels]\n",
    "    #         inputs, labels = data\n",
    "    print(f'\\nEpoch number {epoch}')\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # print(f'i {i}')\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimiser.zero_grad()  # zero parameter gradients\n",
    "        predicted_class = pretrained_vit(inputs)\n",
    "        # y_pred_class = torch.argmax(torch.softmax(predicted_class, dim=1), dim=1)\n",
    "        # # inputs = feature_extractor(images=inputs, return_tensors=\"pt\")\n",
    "        # # outputs = net_tiny(inputs)  # forward\n",
    "        # # logits = outputs.logits # doesn't have `logits`\n",
    "        # # predicted_class_idx = logits.argmax(-1).item()\n",
    "        # # print(\"Predicted class:\", net.config.id2label[predicted_class_idx])\n",
    "        # # outputs = outputs.pooler_output  # get logits out. Shape (20, 192)\n",
    "        # print(f'predicted_class {predicted_class}')\n",
    "        # print(f'predicted_class.shape {predicted_class.shape}')\n",
    "        # print(f'type(y_pred_class[0]) {type(y_pred_class[0])}')\n",
    "        loss = criterion(predicted_class, labels)\n",
    "        loss.backward()  # backward\n",
    "        cumulative_loss += loss.item()\n",
    "        optimiser.step()  # optimise\n",
    "        _calc_accuracy(predicted_class, ground_truth=labels)\n",
    "\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, cumulative_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # # Save model state at each epoch:\n",
    "    # torch.save({\n",
    "    #             'epoch': epoch,\n",
    "    #             'model_state_dict': net.state_dict(),\n",
    "    #             'optimizer_state_dict': optimiser.state_dict(),\n",
    "    #             'loss': loss,\n",
    "    #             }, f'checkpoint{epoch}.pth')\n",
    "\n",
    "    print(f'loss={loss.item()} at epoch={epoch}')\n",
    "\n",
    "print(f'Completed training for {epochs} epochs.')\n",
    "print(f'Training model for {epochs} epochs took {round(((time() - start) / 60), 4)} mins')\n",
    "\n",
    "tuned_model_dirs = 'saved_models/pretrained_finetuned'\n",
    "if not os.path.exists(tuned_model_dirs): os.makedirs(tuned_model_dirs)\n",
    "# deit_tiny_tuned_path = os.path.join(tuned_model_dirs, 'deit_tiny_vit_tuned.pt')\n",
    "deit_small_tuned_path = os.path.join(tuned_model_dirs, 'deit_small_vit_tuned.pt')\n",
    "# torch.save(net.state_dict(), deit_tiny_tuned_path)\n",
    "torch.save(pretrained_vit.state_dict(), deit_small_tuned_path)\n",
    "print('Trained model saved.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "i-vewmZRo3pd",
    "outputId": "986219be-426b-43ec-d3d6-018e8b7740c3",
    "executionInfo": {
     "status": "error",
     "timestamp": 1710801265427,
     "user_tz": 0,
     "elapsed": 896,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    }
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch number 0\n",
      "labels tensor([3, 3, 7, 8, 9, 8, 4, 8, 7, 9, 2, 8, 2, 6, 2, 0, 9, 2, 1, 5])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([3, 7, 3, 1, 7, 4, 8, 4, 7, 8, 6, 5, 6, 1, 1, 9, 7, 4, 1, 0])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([0, 5, 7, 5, 7, 7, 7, 2, 9, 5, 1, 4, 5, 0, 6, 8, 3, 5, 6, 4])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([4, 8, 1, 6, 6, 2, 3, 8, 1, 4, 1, 4, 7, 2, 9, 0, 0, 1, 8, 7])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([9, 0, 4, 1, 0, 9, 3, 3, 6, 9, 5, 2, 5, 9, 0, 7, 6, 0, 0, 3])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([3, 9, 5, 6, 7, 5, 0, 2, 8, 9, 4, 8, 1, 2, 5, 8, 9, 2, 8, 1])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([8, 5, 5, 1, 1, 2, 5, 6, 0, 4, 9, 9, 1, 7, 9, 1, 5, 3, 6, 3])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([6, 4, 5, 2, 5, 9, 8, 0, 3, 2, 1, 7, 1, 7, 9, 9, 6, 2, 4, 5])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([3, 9, 5, 9, 4, 6, 2, 8, 7, 9, 7, 9, 0, 2, 1, 7, 6, 4, 6, 0])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([1, 2, 7, 9, 4, 9, 2, 3, 2, 6, 2, 4, 3, 1, 5, 8, 1, 1, 7, 3])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([7, 6, 5, 8, 4, 6, 2, 4, 7, 3, 7, 7, 0, 1, 7, 5, 1, 4, 5, 7])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([6, 8, 1, 6, 9, 2, 2, 3, 4, 8, 2, 2, 2, 8, 7, 0, 1, 5, 8, 9])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([7, 7, 6, 4, 8, 9, 4, 9, 0, 0, 2, 3, 6, 0, 2, 0, 0, 5, 6, 7])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([2, 4, 6, 7, 2, 7, 3, 4, 1, 2, 6, 0, 9, 9, 9, 0, 7, 1, 4, 9])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([3, 5, 1, 3, 6, 2, 4, 8, 5, 9, 5, 8, 5, 4, 6, 8, 9, 8, 6, 6])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([3, 8, 7, 9, 7, 0, 7, 7, 1, 7, 0, 5, 0, 7, 3, 9, 9, 3, 7, 7])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n",
      "labels tensor([1, 0, 9, 9, 3, 9, 5, 1, 6, 6, 4, 4, 2, 0, 6, 3, 3, 3, 0, 4])\n",
      "predicted_class.shape torch.Size([20, 10])\n",
      "type(labels[0]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [04:45<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 19\u001B[0m\n\u001B[1;32m     17\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device), data[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     18\u001B[0m optimiser\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# zero parameter gradients\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m predicted_class \u001B[38;5;241m=\u001B[39m \u001B[43mpretrained_vit\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# y_pred_class = torch.argmax(torch.softmax(predicted_class, dim=1), dim=1)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# # inputs = feature_extractor(images=inputs, return_tensors=\"pt\")\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# # outputs = net_tiny(inputs)  # forward\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# # outputs = outputs.pooler_output  # get logits out. Shape (20, 192)\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# print(f'predicted_class {predicted_class}')\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:298\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    295\u001B[0m batch_class_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_token\u001B[38;5;241m.\u001B[39mexpand(n, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    296\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([batch_class_token, x], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 298\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001B[39;00m\n\u001B[1;32m    301\u001B[0m x \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:157\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    155\u001B[0m torch\u001B[38;5;241m.\u001B[39m_assert(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_embedding\n\u001B[0;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torchvision/models/vision_transformer.py:113\u001B[0m, in \u001B[0;36mEncoderBlock.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    111\u001B[0m torch\u001B[38;5;241m.\u001B[39m_assert(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    112\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln_1(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m--> 113\u001B[0m x, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[1;32m    115\u001B[0m x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/modules/activation.py:1241\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   1227\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[1;32m   1228\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[1;32m   1229\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1238\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[1;32m   1239\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[1;32m   1240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1241\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1250\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[1;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[0;32m~/miniconda3/envs/comp0197-cw1-pt/lib/python3.12/site-packages/torch/nn/functional.py:5479\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[1;32m   5476\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001B[1;32m   5477\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(bsz \u001B[38;5;241m*\u001B[39m tgt_len, embed_dim)\n\u001B[0;32m-> 5479\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_proj_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   5480\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mview(tgt_len, bsz, attn_output\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m   5481\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[1;32m   5482\u001B[0m     \u001B[38;5;66;03m# squeeze the output if input was unbatched\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Save fine-tuned pretrained DeiT-tiny model:**"
   ],
   "metadata": {
    "id": "Vd09y_C3L2ab"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tuned_model_dirs = 'saved_models/pretrained_finetuned'\n",
    "if not os.path.exists(tuned_model_dirs): os.makedirs(tuned_model_dirs)\n",
    "deit_tiny_tuned_path = os.path.join(tuned_model_dirs, 'deit_tiny_vit_tuned.pt')\n",
    "torch.save(net.state_dict(), deit_tiny_tuned_path))\n",
    "print('Trained model saved.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQZyDR0i3DQv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1710267102733,
     "user_tz": 0,
     "elapsed": 37620,
     "user": {
      "displayName": "Shahin Zibaee",
      "userId": "09120442428585084024"
     }
    },
    "outputId": "0bdb8985-4726-49f2-fe73-ed8e17829f8d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "adl_cw_pt",
   "language": "python",
   "display_name": "comp0197-cw1-pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
