"""
Task 1 Stochastic Minibatch Gradient Descent for Linear Models

Implement a polynomial function polynomial_fun, that takes two input arguments, a weight vector ğ°
of size ğ‘€ + 1 and an input scalar variable ğ‘¥, and returns the function value ğ‘¦.
The polynomial_fun should be vectorised for multiple pairs of scalar input and output,
with the same ğ°. [5]

Using the linear algebra modules in TensorFlow/PyTorch, implement a least square solver for fitting
the polynomial functions, fit_polynomial_ls, which takes ğ‘ pairs of ğ‘¥ and target values ğ‘¡ as input, with
an additional input argument to specify the polynomial degree ğ‘€, and returns the optimum weight
vector hat_ğ° Ì‚in least-square sense, i.e. â€–ğ‘¡ âˆ’ ğ‘¦â€–2 is minimised. [5]

Using relevant functions/modules in TensorFlow/PyTorch, implement a stochastic minibatch gradient
descent algorithm for fitting the polynomial functions, fit_polynomial_sgd, which has the same input
arguments as fit_polynomial_ls does, with additional two input arguments, learning rate and
minibatch size. This function also returns the optimum weight vector hat_ğ°. During training, the function
should report the loss periodically using printed messages. [5]

Implement a task script â€œtask.pyâ€, under folder â€œtask1â€, performing the following: [15]

o Use polynomial_fun (ğ‘€ = 2, ğ° = [1,2,3]T) to generate a training set and a test set, in the
form of respectively and uniformly sampled 20 and 10 pairs of ğ‘¥, ğ‘¥ğœ–[âˆ’20, 20], and ğ‘¡. The
observed ğ‘¡ values are obtained by adding Gaussian noise (standard deviation being 0.5) to ğ‘¦.
o Use fit_polynomial_ls (ğ‘€ğœ–{2,3,4}) to compute the optimum weight vector hat_w using the training set.
For each ğ‘€, compute the predicted target values ğ‘¦ Ì‚ for all ğ‘¥ in both the training
and test sets.

o Report, using printed messages, the mean (and standard deviation) in difference a) between
the observed training data and the underlying â€œtrueâ€ polynomial curve; and b) between the
â€œLS-predictedâ€ values and the underlying â€œtrueâ€ polynomial curve.

o Use fit_polynomial_sgd (ğ‘€ğœ–{2,3,4}) to optimise the weight vector ğ°
Ì‚ using the training set.
For each ğ‘€, compute the predicted target values ğ‘¦
Ì‚ for all ğ‘¥ in both the training and test sets.

o Report, using printed messages, the mean (and standard deviation) in difference between the
â€œSGD-predictedâ€ values and the underlying â€œtrueâ€ polynomial curve.

o Compare the accuracy of your implementation using the two methods with ground-truth on
test set and report the root-mean-square-errors (RMSEs) in both ğ° and ğ‘¦ using printed
messages.

o Compare the speed of the two methods and report time spent in fitting/training (in seconds)
using printed messages.
â€¢ Implement a task script â€œtask1a.pyâ€, under folder â€œtask1â€. [10]

o Experiment how to make ğ‘€ a learnable model parameter and using SGD to optimise this more
flexible model.
o Report, using printed messages, the optimised ğ‘€ value and the mean (and standard deviation) in
difference between the model-predicted values and the underlying â€œtrueâ€ polynomial curve.
"""